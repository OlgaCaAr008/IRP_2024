# -*- coding: utf-8 -*-
"""4layers.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/196O00icQdljGgj9DFbeAkFjQwYivZAjX
"""

import os
from google.colab import drive
drive.mount('/content/drive')
project_folder = '/content/drive/My Drive/Colab Notebooks/ARCHER2_RUNS/results' # working folder path
os.chdir(project_folder) # changing the path

import numpy as np
from scipy.ndimage import distance_transform_edt
import tensorflow as tf
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras import layers, models, regularizers

field = np.load('field.npy') # n x Ny x Nx x 3
geometry = np.load('geo.npy') # n x Ny x Nx

vx = field[:,:,:,1] # n x Ny x Nx
vy = field[:,:,:,2] # n x Ny x Nx
rho = field[:,:,:,0]

# Neural Network

input_layer = layers.Input(shape=(geometry.shape[1], geometry.shape[2], 1))

# Conv 1
x = layers.Conv2D(32, (5, 5), activation='relu', padding='same')(input_layer)
x = layers.MaxPooling2D((2, 2))(x)

# Conv 2
x = layers.Conv2D(64, (5, 5), activation='relu', padding='same')(x)
x = layers.MaxPooling2D((2, 2))(x)

# Conv 3
x = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)
x = layers.MaxPooling2D((2, 2))(x)

# Conv 4
x = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(x)
x = layers.MaxPooling2D((2, 2))(x)

# Flatten the output to feed into the dense layers
x = layers.Flatten()(x)
x = layers.Dense(256, activation='relu')(x)

# Output layers for the x and y components of the velocity
output_x = layers.Dense(vx.shape[1] * vx.shape[2], activation='linear', name='velocity_x')(x)
output_y = layers.Dense(vy.shape[1] * vy.shape[2], activation='linear', name='velocity_y')(x)
output_rho = layers.Dense(rho.shape[1] * rho.shape[2], activation='linear', name='density')(x)

# Create the model
model = models.Model(inputs=input_layer, outputs=[output_x, output_y, output_rho])

model.summary()

early_stopping = EarlyStopping(monitor='val_loss', patience=500, verbose=1, restore_best_weights=True)

model.compile(
    optimizer='adam',
    loss='mean_squared_error',  # Switched from custom_loss_function to mean_squared_error
    metrics={'velocity_x': ['mae'], 'velocity_y': ['mae'], 'density' : ['mae']}
)

Input_data = geometry.reshape((geometry.shape[0], geometry.shape[1], geometry.shape[2], 1))
Vx_data = vx.reshape((vx.shape[0], vx.shape[1] * vx.shape[2]))
Vy_data = vy.reshape((vy.shape[0], vy.shape[1] * vy.shape[2]))
Rho_data = rho.reshape((rho.shape[0], rho.shape[1] * rho.shape[2]))

np.random.seed(42)  # For reproducibility
indices = np.arange(Input_data.shape[0])
np.random.shuffle(indices)

train_size = int(0.8 * len(indices))
train_indices = indices[:train_size]
test_indices = indices[train_size:]

Input_train = Input_data[train_indices]
Input_test = Input_data[test_indices]

Vx_train = Vx_data[train_indices]
Vx_test = Vx_data[test_indices]

Vy_train = Vy_data[train_indices]
Vy_test = Vy_data[test_indices]

Rho_train = Rho_data[train_indices]
Rho_test = Rho_data[test_indices]

history = model.fit(Input_train, {'velocity_x': Vx_train, 'velocity_y': Vy_train, 'density':Rho_train}, epochs=10000, batch_size=32, validation_split=0.2, callbacks=[early_stopping])

test_scores = model.evaluate(Input_test, [Vx_test, Vy_test, Rho_test])

# Save predictions
np.save('test_scores_all35.npy',test_scores[0])

loss = np.array(history.history['loss'])
val_loss = np.array(history.history['val_loss'])

np.save('loss_all35.npy',loss[-1])
np.save('val_loss_all35.npy',val_loss[-1]))

stopped_epoch = early_stopping.stopped_epoch
print("Training was stopped at epoch - all 35:", stopped_epoch)

from tensorflow.keras.models import Model
import numpy as np

# Create a list to hold models for each convolutional layer's output
intermediate_layer_models = []

# Extract the outputs of each convolutional layer
layer_outputs = [layer.output for layer in model.layers if 'conv' in layer.name]

# Create a model for each layer output
for output in layer_outputs:
    intermediate_model = Model(inputs=model.input, outputs=output)
    intermediate_layer_models.append(intermediate_model)

# Choose a sample input image from the test set
input_sample = Input_test[1:2]  # Ensure it's a batch of one

# Dictionary to hold output feature maps for each layer
output_feature_maps = {}

for i, intermediate_model in enumerate(intermediate_layer_models):
    # Get the output of the intermediate model
    outputs = intermediate_model.predict(input_sample)
    # Save outputs to dictionary
    output_feature_maps[f"layer_{i}"] = outputs

# Save the feature maps to a file
np.savez('feature_maps_all35.npz', **output_feature_maps)