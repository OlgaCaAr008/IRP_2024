# -*- coding: utf-8 -*-
"""Copia de Copia de 03NNtest.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1a_jMagnBjSNMNH8LY4KjC9qIDVHVHtoC
"""

# Define the working directory
import os
from google.colab import drive
drive.mount('/content/drive')
project_folder = '/content/drive/My Drive/Colab Notebooks/NN/Test case/1-1001' # working folder path
os.chdir(project_folder) # changing the path

import scipy.io
import numpy as np
import tensorflow as tf
from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense
from tensorflow.keras.models import Model

import matplotlib.pyplot as plt
import datetime

def load_data(geometries, vx, vy, num_samples):

    for i in range(1, num_samples+1):
        geom_data = scipy.io.loadmat(f'geom{i}.mat')['obst']
        vx_data = scipy.io.loadmat(f'ux{i}.mat')['ux']
        vy_data = scipy.io.loadmat(f'uy{i}.mat')['uy']

        geometries.append(geom_data)
        vx.append(vx_data)
        vy.append(vy_data)
    return geometries, vx, vy


geometries = []
vx = []
vy = []

num_samples = 1000

Input_train, Vx_train, Vy_train = load_data(geometries, vx, vy, num_samples) # n x Nx x Ny

 # Convert lists to numpy arrays for training
Input_train = np.array(Input_train)
Vx_train = np.array(Vx_train)
Vy_train = np.array(Vy_train)

Vx_train[np.isnan(Vx_train)] = 0
Vy_train[np.isnan(Vy_train)] = 0
Input_train[np.isnan(Input_train)] = 0

# Input layer
input_layer = Input(shape=(Input_train.shape[1], Input_train.shape[2], 1))

# Convolutional layers
x = Conv2D(32, (3, 3), activation='relu')(input_layer)
x = MaxPooling2D((2, 2))(x)
x = Conv2D(64, (3, 3), activation='relu')(x)
x = MaxPooling2D((2, 2))(x)
x = Conv2D(64, (3, 3), activation='relu')(x)

# Flattening the output of the conv layers to feed into the dense layer
x = Flatten()(x)
x = Dense(64, activation='relu')(x)

# Output layers for the x and y components of the velocity
output_x = Dense(Vx_train.shape[1] * Vx_train.shape[2], activation='linear', name='velocity_x')(x)
output_y = Dense(Vy_train.shape[1] * Vy_train.shape[2], activation='linear', name='velocity_y')(x)

# Create the model
model = Model(inputs=input_layer, outputs=[output_x, output_y])

model.summary()

from tensorflow import keras

# Assuming 'model' is already defined and compiled

# Enhanced model plot with additional details
keras.utils.plot_model(
    model,
    to_file="arch1.png",  # Save the diagram to a file
    show_shapes=True,           # Show input and output shapes for each layer
    expand_nested=True,         # Expand nested models into clusters
    dpi=300                      # Set the resolution of the image
)

def custom_loss_function(y_true, y_pred):
    # Assuming y_true and y_pred are lists of [vx, vy] as the model outputs two tensors
    vx_true, vy_true = y_true[0], y_true[1]
    vx_pred, vy_pred = y_pred[0], y_pred[1]

    # Calculate MSE for each component
    mse_x = tf.reduce_mean(tf.square(vx_true - vx_pred))
    mse_y = tf.reduce_mean(tf.square(vy_true - vy_pred))

    mse_loss = mse_x + mse_y

    vx_pred_reshaped = tf.reshape(vx_pred, [-1, 400, 100])
    vy_pred_reshaped = tf.reshape(vy_pred, [-1, 400, 100])

    du_x_dx = vx_pred_reshaped[:, :, 1:] - vx_pred_reshaped[:, :, :-1]
    du_y_dy = vy_pred_reshaped[:, 1:, :] - vy_pred_reshaped[:, :-1, :]

    # Zero padding for missing boundaries to maintain shape consistency
    du_x_dx = tf.pad(du_x_dx, [[0, 0], [0, 0], [0, 1]], mode='CONSTANT')
    du_y_dy = tf.pad(du_y_dy, [[0, 0], [0, 1], [0, 0]], mode='CONSTANT')

    # Calculate divergence and its MSE loss
    divergence = du_x_dx + du_y_dy
    divergence_loss = tf.reduce_mean(tf.square(divergence))

    # Combine MSE loss with divergence loss, using a weighting factor for the divergence
    lambda_div = 0.1  # This can be adjusted based on how strongly you want to enforce the divergence constraint
    total_loss = mse_loss + lambda_div * divergence_loss
    return total_loss

#model.compile(    optimizer='adam',    loss=custom_loss_function,    metrics={'velocity_x': ['mae'], 'velocity_y': ['mae']})

model.compile(
    optimizer='adam',
    loss='mean_squared_error',  # Switched from custom_loss_function to mean_squared_error
    metrics={'velocity_x': ['mae'], 'velocity_y': ['mae']}
)

Input_train = Input_train.reshape((Input_train.shape[0], Input_train.shape[1], Input_train.shape[2], 1))
Vx_train = Vx_train.reshape((Vx_train.shape[0], Vx_train.shape[1] * Vx_train.shape[2]))
Vy_train = Vy_train.reshape((Vy_train.shape[0], Vy_train.shape[1] * Vy_train.shape[2]))

model.fit(Input_train, {'velocity_x': Vx_train, 'velocity_y': Vy_train}, epochs=10, batch_size=32, validation_split=0.2)
#test_scores = model.evaluate(x_test, y_test, verbose=2)
#print("Test loss:", test_scores[0])
#print("Test accuracy:", test_scores[1])

from tensorflow import keras

keras.utils.plot_model(model, "mini_resnet.png", show_shapes=True)

new_geom = scipy.io.loadmat('geom1001.mat')['obst']
new_geom = new_geom.reshape((1, new_geom.shape[0], new_geom.shape[1], 1))  # Reshape for prediction

# Predicting using the model
predicted_velocities = model.predict(new_geom)
predicted_velocity_x, predicted_velocity_y = predicted_velocities

# Reshaping the output back to the original spatial dimensions
predicted_velocity_x = predicted_velocity_x.reshape((400, 100))
predicted_velocity_y = predicted_velocity_y.reshape((400, 100))

def Velocity_mag(vx_mat,vy_mat):
  magnitude = np.sqrt(vx_mat**2 + vy_mat**2)
  return magnitude

#absol = Velocity_mag(Vx_train[1],Vy_train[1])

def compute_divergence(u_x, u_y):
    du_x_dx = np.zeros_like(u_x)
    du_x_dx[:, :-1] = np.diff(u_x, axis=1)  # forward difference on x-axis

    du_y_dy = np.zeros_like(u_y)
    du_y_dy[:-1, :] = np.diff(u_y, axis=0)  # forward difference on y-axis

    divergence = du_x_dx + du_y_dy

    return divergence

# Load the .mat file
mat_contents = scipy.io.loadmat('ux1001.mat')
u_data = mat_contents['ux']

mat_contents2 = scipy.io.loadmat('uy1001.mat')
v_data = mat_contents2['uy']


plot_real= Velocity_mag(u_data,v_data)
plot_predicted= Velocity_mag(predicted_velocity_x,predicted_velocity_y)


# Figure plot
fig, axs = plt.subplots(1, 2, figsize=(12, 6))
# Plot original data
cax1 = axs[0].imshow(plot_real, interpolation='nearest', cmap='viridis')
axs[0].set_title('Original Velocity')
fig.colorbar(cax1, ax=axs[0], orientation='vertical')
# Plot predicted data
cax2 = axs[1].imshow(plot_predicted, interpolation='nearest', cmap='viridis')
axs[1].set_title('Predicted Velocity')
fig.colorbar(cax2, ax=axs[1], orientation='vertical')

plt.show()

import matplotlib.pyplot as plt
import scipy.io
import numpy as np

# Function to compute velocity magnitude (assuming it's defined elsewhere)
def Velocity_mag(u, v):
    return np.sqrt(u**2 + v**2)

# Load the .mat files
mat_contents = scipy.io.loadmat('ux1001.mat')
u_data = mat_contents['ux']

mat_contents2 = scipy.io.loadmat('uy1001.mat')
v_data = mat_contents2['uy']

plot_real = Velocity_mag(u_data, v_data)
plot_predicted = Velocity_mag(predicted_velocity_x, predicted_velocity_y)

# Calculate the difference
plot_difference = plot_real - plot_predicted

# Figure plot with 3 subplots
fig, axs = plt.subplots(1, 3, figsize=(10, 6))

# Plot original data
cax1 = axs[0].imshow(plot_real, interpolation='nearest', cmap='viridis')
axs[0].set_title('Original Velocity')
fig.colorbar(cax1, ax=axs[0], orientation='vertical')
axs[0].set_xlabel('y dimension')
axs[0].set_ylabel('x dimension')

# Plot predicted data
cax2 = axs[1].imshow(plot_predicted, interpolation='nearest', cmap='viridis')
axs[1].set_title('Predicted Velocity')
fig.colorbar(cax2, ax=axs[1], orientation='vertical')
axs[1].set_xlabel('y dimension')
axs[1].set_ylabel('x dimension')

# Plot difference data
cax3 = axs[2].imshow(plot_difference, interpolation='nearest', cmap='viridis')
axs[2].set_title('Difference (Original - Predicted)')
fig.colorbar(cax3, ax=axs[2], orientation='vertical')
axs[2].set_xlabel('y dimension')
axs[2].set_ylabel('x dimension')

# Adjust subplot parameters
plt.subplots_adjust(left=0.05, right=0.95, wspace=0.2)

plt.savefig('arch1Vel.png', dpi=300)  # Specify the DPI for higher resolution

relative_error = np.abs(plot_real - plot_predicted) / (np.abs(plot_real) + 1e-10)
np.nanmean(relative_error)*100

mean_value = np.nanmean(np.abs(plot_real-plot_predicted))
mean_value